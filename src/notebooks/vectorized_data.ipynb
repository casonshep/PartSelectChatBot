{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "42f09ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (3.9.1)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (4.0.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (4.51.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: click in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from sentence_transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from sentence_transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from sentence_transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from sentence_transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from sentence_transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence_transformers) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shepa\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\shepa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tensorflow\n",
    "%pip install pandas nltk sentence_transformers transformers\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt', download_dir=nltk.data.path[0])\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844d8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shepa\\AppData\\Local\\Temp\\ipykernel_28924\\2310578413.py:22: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_cleaned = combined_df.applymap(clean_html)\n",
      "C:\\Users\\shepa\\AppData\\Local\\Temp\\ipykernel_28924\\2310578413.py:13: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  text = BeautifulSoup(raw_html, 'html.parser').get_text(separator=' ', strip=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nf__links href</th>\n",
       "      <th>BODY</th>\n",
       "      <th>container</th>\n",
       "      <th>repair-story</th>\n",
       "      <th>section-title</th>\n",
       "      <th>repair-story (2)</th>\n",
       "      <th>repair-story (3)</th>\n",
       "      <th>container (2)</th>\n",
       "      <th>d-flex</th>\n",
       "      <th>d-flex (2)</th>\n",
       "      <th>...</th>\n",
       "      <th>d-flex (4)</th>\n",
       "      <th>d-flex (5)</th>\n",
       "      <th>no-underline href</th>\n",
       "      <th>schematic-thump__img src</th>\n",
       "      <th>no-underline href (2)</th>\n",
       "      <th>schematic-thump__img src (2)</th>\n",
       "      <th>no-underline href (3)</th>\n",
       "      <th>schematic-thump__img src (3)</th>\n",
       "      <th>no-underline href (4)</th>\n",
       "      <th>schematic-thump__img src (4)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.partselect.com/Refrigerator-Trays-...</td>\n",
       "      <td>Skip to main content 1-866-319-8402 Monday to ...</td>\n",
       "      <td>Refrigerator Trays and Shelves Find Your Part ...</td>\n",
       "      <td>In Stock Item is in stock and will ship today ...</td>\n",
       "      <td>In Stock Item is in stock and will ship today ...</td>\n",
       "      <td>In Stock Item is in stock and will ship today ...</td>\n",
       "      <td>Feedback How would you rate your experience? H...</td>\n",
       "      <td>Refrigerator Trays and Shelves Find Your Part ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      nf__links href  \\\n",
       "0  https://www.partselect.com/Refrigerator-Trays-...   \n",
       "\n",
       "                                                BODY  \\\n",
       "0  Skip to main content 1-866-319-8402 Monday to ...   \n",
       "\n",
       "                                           container  \\\n",
       "0  Refrigerator Trays and Shelves Find Your Part ...   \n",
       "\n",
       "                                        repair-story  \\\n",
       "0  In Stock Item is in stock and will ship today ...   \n",
       "\n",
       "                                       section-title  \\\n",
       "0  In Stock Item is in stock and will ship today ...   \n",
       "\n",
       "                                    repair-story (2)  \\\n",
       "0  In Stock Item is in stock and will ship today ...   \n",
       "\n",
       "                                    repair-story (3)  \\\n",
       "0  Feedback How would you rate your experience? H...   \n",
       "\n",
       "                                       container (2) d-flex d-flex (2)  ...  \\\n",
       "0  Refrigerator Trays and Shelves Find Your Part ...                    ...   \n",
       "\n",
       "  d-flex (4) d-flex (5) no-underline href schematic-thump__img src  \\\n",
       "0                                                                    \n",
       "\n",
       "  no-underline href (2) schematic-thump__img src (2) no-underline href (3)  \\\n",
       "0                                                                            \n",
       "\n",
       "  schematic-thump__img src (3) no-underline href (4)  \\\n",
       "0                                                      \n",
       "\n",
       "  schematic-thump__img src (4)  \n",
       "0                               \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load your CSV\n",
    "fridge_data = r'data\\refridgerator_scrape.csv'\n",
    "dishwasher_data = r'data\\dishwasher_scrape.csv'\n",
    "df = pd.read_csv(fridge_data)\n",
    "df1 = pd.read_csv(dishwasher_data)\n",
    "combined_df = pd.concat([df, df1], ignore_index=True, axis=0)\n",
    "# Function to clean HTML and remove escape characters\n",
    "def clean_html(raw_html):\n",
    "    if pd.isna(raw_html):\n",
    "        return ''\n",
    "    \n",
    "    # Strip HTML tags\n",
    "    text = BeautifulSoup(raw_html, 'html.parser').get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Remove \\r, \\n, \\t and extra spaces\n",
    "    text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply to all columns\n",
    "df_cleaned = combined_df.applymap(clean_html)\n",
    "df_cleaned.to_csv(\"test.csv\", index=False)\n",
    "# Show the cleaned DataFrame\n",
    "df_cleaned.head(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repair_stories = pd.read_csv(r'\\data\\repair_stories.csv')\n",
    "repair_stories.head()\n",
    "\n",
    "parts = pd.read_csv(r'\\data\\part_info_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e641d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(row):\n",
    "    # Iterate through all columns and concatenate the text values\n",
    "    return ' | '.join([f\"{col}: {row[col]}\" for col in row.index if isinstance(row[col], str) and row[col].strip() != \"\"])\n",
    "\n",
    "df_cleaned['combined_text'] = df_cleaned.apply(combine_columns, axis=1)\n",
    "parts['combined_text'] = parts.apply(combine_columns, axis=1)\n",
    "df_cleaned = df_cleaned[['combined_text']]\n",
    "parts_cleaned = parts[['combined_text']]\n",
    "\n",
    "repair_stories.rename(columns={'repair-story': 'combined_text'}, inplace=True)\n",
    "\n",
    "full_text = pd.concat((df_cleaned, parts_cleaned, repair_stories), axis=0)\n",
    "full_text.head()\n",
    "full_text = full_text.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c01f11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nf__links href: https://www.partselect.com/Ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nf__links href: https://www.partselect.com/Ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nf__links href: https://www.partselect.com/Ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nf__links href: https://www.partselect.com/Ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nf__links href: https://www.partselect.com/Ref...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       combined_text\n",
       "0  nf__links href: https://www.partselect.com/Ref...\n",
       "1  nf__links href: https://www.partselect.com/Ref...\n",
       "2  nf__links href: https://www.partselect.com/Ref...\n",
       "3  nf__links href: https://www.partselect.com/Ref...\n",
       "4  nf__links href: https://www.partselect.com/Ref..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text['combined_text'] = full_text['combined_text'].replace({r'[\\n\\t⭐]': ' '}, regex=True)\n",
    "full_text['combined_text'] = full_text['combined_text'].replace({'★★★★★': ''}, regex=False)\n",
    "\n",
    "full_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cca8dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer('BAAI/bge-small-en')\n",
    "\n",
    "# Load the BERT tokenizer (you can change this to a different BERT variant)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Download NLTK punkt tokenizer\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Function to split the text into chunks\n",
    "def split_text_into_chunks(text, max_sentences_per_chunk=3):\n",
    "    # Split text into sentences (naive split based on period)\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    \n",
    "    # Create chunks where each chunk has a maximum of `max_sentences_per_chunk` sentences\n",
    "    chunks = ['. '.join(sentences[i:i + max_sentences_per_chunk]) + '.' for i in range(0, len(sentences), max_sentences_per_chunk)]\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create lists for chunks and metadata\n",
    "processed_chunks = []\n",
    "\n",
    "\n",
    "chunk_set = set()\n",
    "for idx, row in full_text.iterrows():\n",
    "    combined_text = row['combined_text']\n",
    "    \n",
    "    # Split into 2–3 chunks\n",
    "    chunks = split_text_into_chunks(combined_text)\n",
    "    chunk_set.update(chunks)\n",
    "\n",
    "    \n",
    "for chunk_id, chunk in enumerate(chunk_set):\n",
    "    tokens = tokenizer.tokenize(chunk)\n",
    "    processed_chunks.append({\n",
    "        \"chunk_id\": chunk_id,\n",
    "        \"original_text\": chunk,\n",
    "        \"tokenized_text\": tokens\n",
    "    })    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b5d88252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "# index = faiss.read_index(r'data/embedded_chunks.faiss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4688849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 402/402 [05:05<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare original text chunks for embedding\n",
    "all_chunks = [entry[\"original_text\"] for entry in processed_chunks]\n",
    "\n",
    "# Step 2: Embed chunks\n",
    "embeddings = model.encode(all_chunks, show_progress_bar=True)\n",
    "\n",
    "# Step 3: Create FAISS index\n",
    "embedding_matrix = np.vstack(embeddings).astype(\"float32\")\n",
    "dimension = embedding_matrix.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 = cosine similarity if embeddings are normalized\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# Step 4: Save FAISS index\n",
    "faiss.write_index(index, \"embedded_chunks.faiss\")\n",
    "\n",
    "# Step 5: Save metadata (original chunk, row info, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a53351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embedded_chunks_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processed_chunks, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45a88f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12856\n"
     ]
    }
   ],
   "source": [
    "print(len(all_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "07ba3706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12856\n"
     ]
    }
   ],
   "source": [
    "print(index.ntotal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
